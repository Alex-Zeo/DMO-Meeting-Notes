{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0FrNES6qMEUH/0NSqz5Q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Zeo/DMO-Meeting-Notes/blob/main/DMO_Whisper_Large_Meeting_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcribe Youtube Videos\n",
        "This script automates transcribing Youtube videos by trying two approaches:\n",
        "\n",
        "1) First it checks if Youtube has a transcript file available for download\n",
        "\n",
        "2) Next it downloads the video audio and uses OpenAi Whisper model to generate a transcript from the audio\n",
        "\n",
        "This script will save both transcripts (if one exists on youtube), and logs the results."
      ],
      "metadata": {
        "id": "5qoV386pA1MB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjb-Ev-mcQye",
        "outputId": "545b57fc-6f91-48ac-b03e-b16584bf6fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [60.9 kB]\n",
            "Err:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages\n",
            "  File has unexpected size (59493 != 60939). Mirror sync in progress? [IP: 3.171.85.81 443]\n",
            "  Hashes of expected file:\n",
            "   - Filesize:60939 [weak]\n",
            "   - SHA512:a24bf2bdac2484a947bd7798072fee156c7ba3ed514a04c91cadc04ae877ac5e7087ed33c46d0bfbc82882ddac9fdc8f7b90235da9efa6b6825849b792b4ac97\n",
            "   - SHA256:da1ec7f7a336da1e24ac2bf5737324342aab90dae5ef116f23cbcb3033930336\n",
            "   - MD5Sum:2649d18b07e0c56407b7df8a4a8de40e [weak]\n",
            "  Release file created at: Mon, 16 Dec 2024 14:15:48 +0000\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,545 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,364 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,627 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,510 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,469 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,765 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,516 kB]\n",
            "Fetched 26.4 MB in 4s (7,262 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Failed to fetch https://cloud.r-project.org/bin/linux/ubuntu/jammy-cran40/Packages.gz  File has unexpected size (59493 != 60939). Mirror sync in progress? [IP: 3.171.85.81 443]\n",
            "   Hashes of expected file:\n",
            "    - Filesize:60939 [weak]\n",
            "    - SHA512:a24bf2bdac2484a947bd7798072fee156c7ba3ed514a04c91cadc04ae877ac5e7087ed33c46d0bfbc82882ddac9fdc8f7b90235da9efa6b6825849b792b4ac97\n",
            "    - SHA256:da1ec7f7a336da1e24ac2bf5737324342aab90dae5ef116f23cbcb3033930336\n",
            "    - MD5Sum:2649d18b07e0c56407b7df8a4a8de40e [weak]\n",
            "   Release file created at: Mon, 16 Dec 2024 14:15:48 +0000\n",
            "E: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "Starting script...\n",
            "Starting video processing...\n",
            "Extracting video ID from URL...\n",
            "Video ID extracted: 6qO1cHnJTz8\n",
            "\n",
            "=== Getting YouTube Transcript ===\n",
            "Checking available transcript languages...\n",
            "Error getting YouTube transcript: 'TranscriptList' object has no attribute '_transcripts'\n",
            "\n",
            "=== Generating Whisper Transcript ===\n",
            "Downloading audio...\n",
            "[youtube] Extracting URL: https://youtu.be/6qO1cHnJTz8\n",
            "[youtube] 6qO1cHnJTz8: Downloading webpage\n",
            "[youtube] 6qO1cHnJTz8: Downloading ios player API JSON\n",
            "[youtube] 6qO1cHnJTz8: Downloading mweb player API JSON\n",
            "[youtube] 6qO1cHnJTz8: Downloading player 2f1832d2\n",
            "[youtube] 6qO1cHnJTz8: Downloading m3u8 information\n",
            "[info] 6qO1cHnJTz8: Downloading 1 format(s): 251\n",
            "[download] Destination: audio\n",
            "[download] 100% of   20.26MiB in 00:00:03 at 6.22MiB/s   \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio (pass -k to keep)\n",
            "Audio downloaded successfully.\n",
            "Loading Whisper large model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.87G/2.87G [01:02<00:00, 49.5MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing audio with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:16.000]  Los Angeles is doing a $1000 gift card giveaway for a two night stay at a Fairmont.\n",
            "[00:16.000 --> 00:24.000]  If you sign up for their, you get put into a raffle for their newsletter.\n",
            "[00:24.000 --> 00:32.000]  Wow, that sounds a little... $1000 raffle.\n",
            "[00:32.000 --> 00:34.000]  For a two night stay.\n",
            "[00:34.000 --> 00:44.000]  We used to do sweepstakes to generate email subscriptions.\n",
            "[00:44.000 --> 01:02.000]  And that was the way we did it for many, many years. And it led to almost 200,000 names in our database.\n",
            "[01:02.000 --> 01:16.000]  And then we stopped doing it and when we converted over to MailChimp, we had to purge over 80,000 of them.\n",
            "[01:16.000 --> 01:20.000]  The problem with these sweepstakes is that you don't get quality signups.\n",
            "[01:20.000 --> 01:26.000]  Yeah. Like I just signed up for it, I'll probably never...\n",
            "[01:26.000 --> 01:31.000]  Exactly. The last thing we need is more Andrew Clarke signing up.\n",
            "[01:31.000 --> 01:34.000]  Yep. They're here today, gone tomorrow.\n",
            "[01:34.000 --> 01:42.000]  Sometimes I just... if that's happening, I have so many emails that I use. I'll just go through and...\n",
            "[01:42.000 --> 01:43.000]  Yeah.\n",
            "[01:43.000 --> 01:45.000]  All my chances.\n",
            "[01:45.000 --> 01:50.000]  Okay. So I don't think we're going to have anybody else join at this point.\n",
            "[01:50.000 --> 01:55.000]  So let's just get going and make this pretty brief.\n",
            "[01:55.000 --> 02:07.000]  My top things, we have our co-lead meeting this afternoon and I'm working on getting that statement of work done for you, Andrew, with the guy for the Google ads.\n",
            "[02:07.000 --> 02:16.000]  Hopefully this week, Joanne. I think we may have to pivot. I'll follow up with you later for the Explore Georgia ad.\n",
            "[02:16.000 --> 02:25.000]  I have other ads that are in the right dimensions. It's just we'll have to link them to the homepage or the Michelin site or something.\n",
            "[02:25.000 --> 02:27.000]  That's fine. Just fine.\n",
            "[02:27.000 --> 02:28.000]  At this point.\n",
            "[02:28.000 --> 02:31.000]  I'll run a show via VIK anyway.\n",
            "[02:31.000 --> 02:39.000]  Andrew and I had a good meeting with the Clip Trips people last week and then I got an email from Travis this weekend.\n",
            "[02:39.000 --> 02:44.000]  So I think he sent it to you as well, Andrew. Or I think it was to both of us.\n",
            "[02:44.000 --> 02:49.000]  So just trying to see how to move forward with Scott on the things that we discussed.\n",
            "[02:49.000 --> 02:52.000]  And also the MyRubble ones.\n",
            "[02:52.000 --> 02:57.000]  Yeah. Good. Well, you mentioned the co-lead meeting.\n",
            "[02:57.000 --> 03:18.000]  This week is going to be my focus is going to be on the Lovefest agenda and wrapping up the year's budget.\n",
            "[03:18.000 --> 03:23.000]  This week, my main priorities is the Max Award submission.\n",
            "[03:23.000 --> 03:30.000]  We got some numbers and I think we're just waiting on some more from Telemundo and the website.\n",
            "[03:30.000 --> 03:34.000]  I'm working on a video that'll go that'll say all the numbers and stuff.\n",
            "[03:34.000 --> 03:40.000]  I'm going to start a script today. So we have that also working on the Lovefest creative video.\n",
            "[03:40.000 --> 03:44.000]  This week, I'll start pulling video clips to add.\n",
            "[03:44.000 --> 03:50.000]  And then tomorrow we have a shoot with Miss Georgia.\n",
            "[03:50.000 --> 03:53.000]  I think that should be all day. We're still waiting on a final itinerary.\n",
            "[03:53.000 --> 03:59.000]  And then ATL Unguided, we've been hearing back from people that are interested in getting some brochures.\n",
            "[03:59.000 --> 04:03.000]  So I'll keep you posted, Joanne.\n",
            "[04:03.000 --> 04:06.000]  And why Miss Georgia?\n",
            "[04:06.000 --> 04:10.000]  Taylor looped me in, I think, while I was away.\n",
            "[04:10.000 --> 04:17.000]  But I think she reached out a while ago. It's kind of like a new year, new you kind of itinerary.\n",
            "[04:17.000 --> 04:21.000]  She's going to take us through like some of her Atlanta favorites, it sounds like.\n",
            "[04:21.000 --> 04:24.000]  Oh, good. As long as she's in Atlanta.\n",
            "[04:24.000 --> 04:31.000]  Definitely in Atlanta. And you're not going to Hey Hira or Sparta or someplace like that.\n",
            "[04:31.000 --> 04:36.000]  Great. Everything's within like the downtown midtown that we discussed.\n",
            "[04:36.000 --> 04:41.000]  So we'll keep you posted.\n",
            "[04:41.000 --> 04:44.000]  That's it for me.\n",
            "[04:44.000 --> 04:50.000]  Good morning. This is a wrap up week for me. So we have our up next debrief today.\n",
            "[04:50.000 --> 04:59.000]  I'll meet with the digital tomorrow for some design work on the platform and just wrapping up RCMA.\n",
            "[04:59.000 --> 05:03.000]  Any of the collateral for that event. This is right when we come back.\n",
            "[05:03.000 --> 05:12.000]  Also working with the sales team to get any pictures in from their event, their industry events this week and at the beginning of the year.\n",
            "[05:12.000 --> 05:19.000]  And that's it for me.\n",
            "[05:19.000 --> 05:30.000]  Good morning for me this week. My priorities are updating the calendar events with our major partner events as well as sporting events.\n",
            "[05:30.000 --> 05:35.000]  I'm also updating our 2025 concerts article.\n",
            "[05:35.000 --> 05:42.000]  I am working on a copy for our meetings pages.\n",
            "[05:42.000 --> 05:48.000]  The comic meeting is today. And then I'm updating some of our seasonal pages for 2025.\n",
            "[05:48.000 --> 05:55.000]  So working on that. And then that's all for me this week.\n",
            "[05:55.000 --> 06:00.000]  And good morning, everybody. And happy birthday, Andrew. For some reason, I thought your birthday was the 18th.\n",
            "[06:00.000 --> 06:06.000]  So, so happy official day today.\n",
            "[06:06.000 --> 06:11.000]  It can be the 18th too, if you like.\n",
            "[06:11.000 --> 06:20.000]  Well, I'm PTOing this week, but I will be joining calls and probably floating in a little bit just for catch up more than anything else.\n",
            "[06:20.000 --> 06:31.000]  But good news, Nobu has accepted our bid to host Lovefest and we've got a great rate with them for accommodations as well.\n",
            "[06:31.000 --> 06:41.000]  So I'll be signing that contract today and getting all the information out to the team for Jalen.\n",
            "[06:41.000 --> 06:46.000]  I think. I'm sorry. Yeah, Andrew. Did you say something?\n",
            "[06:46.000 --> 06:55.000]  Andrew Clark? No? I'd like to and I'll circle back with Jalen Griffith about this.\n",
            "[06:55.000 --> 07:01.000]  Want to see if we could have a last meeting with Ripple's on Cookbook.\n",
            "[07:01.000 --> 07:11.000]  I'm still getting, you know, sales kind of dribbling in. I know we need to confirm retail sales possibilities.\n",
            "[07:11.000 --> 07:23.000]  I believe, did Andrew, the other Andrew, send us a recap of what is possible from a commission to those who sell them, the stores who sell them, etc.\n",
            "[07:23.000 --> 07:30.000]  etc. I cannot. I'm going to see if I can find that. I can't remember if it's Dorothy or Andrew. Jalen, do you remember?\n",
            "[07:30.000 --> 07:35.000]  I'm sorry to get you while you're nibbling.\n",
            "[07:35.000 --> 07:42.000]  I'm sorry. That was rude. So sorry. No worries. I'll find it and circle back with you all about that.\n",
            "[07:42.000 --> 07:52.000]  But I do believe there what we had a long conversation about who signed up, who's interested from a local bookshop, retail establishment.\n",
            "[07:52.000 --> 07:59.000]  And then what are the possibilities for commissions that the retailer would get and more.\n",
            "[07:59.000 --> 08:05.000]  So but I believe it was all sort of this could happen and nothing definitive.\n",
            "[08:05.000 --> 08:14.000]  So a little bit more. I want to get that locked and loaded this week. And then a couple of other just meetings,\n",
            "[08:14.000 --> 08:24.000]  planning to set up from a partnership perspective, SCAD and some others, and then try to wrap up, as Andrew said,\n",
            "[08:24.000 --> 08:39.000]  expenses as well as getting some invoices through accounting, their billing transmittal, and then help with CFP donning event.\n",
            "[08:39.000 --> 08:47.000]  I think that's it. OK, so I have budget. Andrew, I'm going to follow up with Bruno.\n",
            "[08:47.000 --> 08:54.000]  He's he should be finished with all updates. He said he was going to be done with that.\n",
            "[08:54.000 --> 09:00.000]  This week. So I'll follow up with him. Love Fest. We have our next meeting for that.\n",
            "[09:00.000 --> 09:13.000]  It's going to be on Thursday. Everybody should have that on their calendars. Marketing closet, of course.\n",
            "[09:13.000 --> 09:21.000]  Oh, ACO and guided with Aubrey. I know the updates for our strategic plan.\n",
            "[09:21.000 --> 09:27.000]  Let's get those in so we're not like rushing when we get back from the break.\n",
            "[09:27.000 --> 09:38.000]  Make sure you do that. And then just following up with Geiger on promo items and then getting some of those bins out of our marketing closet.\n",
            "[09:38.000 --> 09:49.000]  And invoices.\n",
            "[09:49.000 --> 09:59.000]  We spy alone. Is that me? Happy Monday, everybody. Happy birthday, Andrew.\n",
            "[09:59.000 --> 10:10.000]  Let's see. I'm taking PTO this week, mainly just to burn it because I'm going to lose it otherwise. But I definitely have several client calls.\n",
            "[10:10.000 --> 10:17.000]  They want to get it all in this week. So have a handful of those. Obviously, CFP as well.\n",
            "[10:17.000 --> 10:29.000]  I don't know if you saw my request and if you were able to connect with Daniel on that, but they are on the hunt for any headshots. And I don't know if we have them.\n",
            "[10:29.000 --> 10:37.000]  I know that was not part of our ask. So I just wanted to be able to provide Mary Beth with an update. I have a call with her in like 30 minutes.\n",
            "[10:37.000 --> 10:49.000]  We should have a few of them, I think, from cookbooks. Yeah, I didn't know if we were actually had permission to share or what.\n",
            "[10:49.000 --> 10:56.000]  And so, Andrew, just so I didn't include you. I had Daniela, Joanne and Frita on the ask.\n",
            "[10:56.000 --> 11:09.000]  But I should know a little bit more once I talk with CFP. But the short of it is they have not even begun to market the taste of that.\n",
            "[11:09.000 --> 11:17.000]  Why? Don't ask me why. I know. Trust me, it took everything in my power to bite my tongue.\n",
            "[11:17.000 --> 11:22.000]  But their thinking is, well, we don't have all the stuff confirmed, so we should wait until we do that.\n",
            "[11:22.000 --> 11:34.000]  And I was like, well, you know, I tried to very politely say, well, I think that it could actually be a great opportunity to create a bit of promo for these chefs that we have not gotten into the confirmed column.\n",
            "[11:34.000 --> 11:40.000]  So let's go ahead and push the ask out and let's let's give assets to those chefs that have graciously agreed to come in already.\n",
            "[11:40.000 --> 11:53.000]  So I think we've gotten them over that. And most likely because of that, that's why CFP and Sports Council is asking for headshots of those that we currently have on the list.\n",
            "[11:53.000 --> 12:00.000]  And we did not make that into our ask when we were making outreach. So that's the reason I just wanted to. Would it be in like Sojan's fault?\n",
            "[12:00.000 --> 12:09.000]  I can look through. I just didn't know where to even start with those. I was not really involved with cookbook.\n",
            "[12:09.000 --> 12:15.000]  But I guess I can dig through creative services and I don't know how much overlap we would have.\n",
            "[12:15.000 --> 12:23.000]  But I want to try and at least give them an update before my call in a few minutes.\n",
            "[12:23.000 --> 12:31.000]  So that's CFP. We've had a lot of calls, honestly, over the past ever since Thursday.\n",
            "[12:31.000 --> 12:39.000]  I think we've got a game plan. Obviously, what we're going to talk about with Sports Council this morning is what can we do to further that?\n",
            "[12:39.000 --> 12:56.000]  Jayla Moore, I sent you a lot of items that are already kind of been announced that we just want to make sure that's added to our calendar of events, including taste, most of which have links to tickets like where to purchase tickets.\n",
            "[12:56.000 --> 13:04.000]  So we can at least do our part and get it on the calendar of events and add more as needed.\n",
            "[13:04.000 --> 13:12.000]  But, yeah, I'll try to hide that, Andrew, if you maybe know where they could reside. I'll pick your brain on that later.\n",
            "[13:12.000 --> 13:20.000]  I'm not sure, Lauren. If the chef is in one of the cookbooks, it means we have a photo.\n",
            "[13:20.000 --> 13:21.000]  Yeah.\n",
            "[13:21.000 --> 13:25.000]  Where that is on the drive, I do not know.\n",
            "[13:25.000 --> 13:28.000]  I know that Sojourn had it pretty well organized.\n",
            "[13:28.000 --> 13:31.000]  Yeah. I'll start with creative services.\n",
            "[13:31.000 --> 13:42.000]  Yeah, because I'm thinking Daniella from Cignia, the chef from the Alden, I believe he's accepted.\n",
            "[13:42.000 --> 13:52.000]  And then we've got PR agency folks, too, that might be able to help with at least a few of the key ones.\n",
            "[13:52.000 --> 13:55.000]  Yeah, totally.\n",
            "[13:55.000 --> 14:00.000]  We can always start. And I'm sorry. I know I joined a little bit late.\n",
            "[14:00.000 --> 14:05.000]  What is everybody else? What is everybody else's PTO schedule like?\n",
            "[14:05.000 --> 14:12.000]  I know Sharitha is kind of in and out this week, but I just don't want to bug people if they're on PTO.\n",
            "[14:12.000 --> 14:20.000]  And I didn't realize that. So do we have maybe Jalen Griffiths, we can get back to our spreadsheet.\n",
            "[14:20.000 --> 14:27.000]  I just always hate to bug people if they're on PTO.\n",
            "[14:27.000 --> 14:30.000]  I, on the other hand, don't seem to understand what PTO is.\n",
            "[14:30.000 --> 14:33.000]  Same here.\n",
            "[14:33.000 --> 14:36.000]  Let's see what else.\n",
            "[14:36.000 --> 14:43.000]  Deja and I just have a couple like housekeeping things with DigiDec.\n",
            "[14:43.000 --> 14:47.000]  We'll have our up next brief today. I'm sure Deja talked about all this earlier.\n",
            "[14:47.000 --> 14:55.000]  Apologies. We were going to do another deeper dive demo with Fabio on the map product.\n",
            "[14:55.000 --> 15:05.000]  Andrew, Sharitha and I will be reviewing the meetings proposal, plan proposal from SparkLoft later today.\n",
            "[15:05.000 --> 15:07.000]  What time is that meeting?\n",
            "[15:07.000 --> 15:18.000]  It's at one, but I'd love to connect with y'all even for like 10 minutes before, just so that we're all aligned on feedback.\n",
            "[15:18.000 --> 15:22.000]  So I'll ping y'all on that.\n",
            "[15:22.000 --> 15:26.000]  I think, like I mentioned, I do have several client calls.\n",
            "[15:26.000 --> 15:30.000]  I've got a call with Skills later this week.\n",
            "[15:30.000 --> 15:37.000]  And then I will have a kickoff call for MLB All-Star and some of the partners there.\n",
            "[15:37.000 --> 15:38.000]  So I'll bring y'all up to speed.\n",
            "[15:38.000 --> 15:42.000]  Most likely, this is just cramming it in before the holiday.\n",
            "[15:42.000 --> 15:49.000]  And once I understand a little bit more of what they're looking to accomplish and where I think we can really plug in,\n",
            "[15:49.000 --> 15:54.000]  I will absolutely be pulling in more people.\n",
            "[15:54.000 --> 15:56.000]  I'm just going to vet it.\n",
            "[15:56.000 --> 16:01.000]  I'll spare y'all, it's a four o'clock call on a Wednesday.\n",
            "[16:01.000 --> 16:05.000]  And then I feel like that's everything.\n",
            "[16:05.000 --> 16:07.000]  If I'm missing anything, let me know.\n",
            "[16:07.000 --> 16:18.000]  But mainly for me, I'm focusing on getting any big items wrapped up so that I can kind of unplug for a little bit.\n",
            "[16:18.000 --> 16:23.000]  But I'll continue, CFP will continue to be babysat throughout the holiday.\n",
            "[16:23.000 --> 16:26.000]  I mean, that's just where we are.\n",
            "[16:26.000 --> 16:27.000]  Yeah.\n",
            "[16:27.000 --> 16:30.000]  And I think we're almost done with Planner's Guide, I believe.\n",
            "[16:30.000 --> 16:31.000]  I think so.\n",
            "[16:31.000 --> 16:32.000]  Yeah.\n",
            "[16:31.000 --> 16:32.000]  Yeah.\n",
            "[16:32.000 --> 16:33.000]  I think we're in a good spot.\n",
            "[16:33.000 --> 16:37.000]  I'm going to touch base with Julie on that this morning.\n",
            "[16:37.000 --> 16:42.000]  But, yeah, I actually do feel like we're in a good spot.\n",
            "[16:42.000 --> 16:51.000]  And, Jalen, I feel like we're plugging through with the promo items.\n",
            "[16:51.000 --> 16:58.000]  One other thing, we are in the process of ordering some higher-end branded, like, lanyards.\n",
            "[16:58.000 --> 17:00.000]  We're doing this for sales and service.\n",
            "[17:00.000 --> 17:10.000]  They want to have them kind of in their closet so that they can take to industry events and other things that they do so that they're not, you know,\n",
            "[17:10.000 --> 17:15.000]  they're using our own versus some of the ones that may be branded at these shows.\n",
            "[17:15.000 --> 17:20.000]  And I don't know if there's a need for us to have any.\n",
            "[17:20.000 --> 17:24.000]  So once I get the virtual mockup, I can share.\n",
            "[17:24.000 --> 17:30.000]  But just putting that out there in case that may be something that we want to have on our shelves, too.\n",
            "[17:30.000 --> 17:34.000]  So it really is not meant for giveaways or anything.\n",
            "[17:34.000 --> 17:37.000]  It's more for, like, staff use.\n",
            "[17:37.000 --> 17:42.000]  Would they be available in time for Lovefest?\n",
            "[17:42.000 --> 17:49.000]  Possibly because we're trying to get them in time for PCMA, which is before Lovefest or right around the time of Lovefest.\n",
            "[17:49.000 --> 17:50.000]  Okay.\n",
            "[17:50.000 --> 17:52.000]  Well, Jaylen, make a note of that.\n",
            "[17:52.000 --> 17:55.000]  We need to do name badges.\n",
            "[17:55.000 --> 17:58.000]  Lanyards would be nice.\n",
            "[17:58.000 --> 17:59.000]  Yeah, these are really nice.\n",
            "[17:59.000 --> 18:04.000]  They've got, like, metal clasp and leather, like, leather touches to them.\n",
            "[18:04.000 --> 18:06.000]  I think they're really nice.\n",
            "[18:06.000 --> 18:12.000]  But they're not, for the price point, I mean, they're, like, $8 a piece, which is not crazy.\n",
            "[18:12.000 --> 18:19.000]  I mean, certainly not as inexpensive as, you know, the cheapo ones.\n",
            "[18:19.000 --> 18:23.000]  But I think we're going to get a better price for doing a larger order.\n",
            "[18:23.000 --> 18:25.000]  So, yeah, Jaylen, I'll keep you posted.\n",
            "[18:25.000 --> 18:28.000]  I'm in plan contact with the guy.\n",
            "[18:28.000 --> 18:32.000]  But, yeah, I think that's it.\n",
            "[18:32.000 --> 18:33.000]  All right.\n",
            "[18:33.000 --> 18:34.000]  Alex, welcome back from BTO.\n",
            "[18:34.000 --> 18:39.000]  You got anything to share for the week?\n",
            "[18:39.000 --> 18:42.000]  Hello.\n",
            "[18:42.000 --> 18:43.000]  Not much.\n",
            "[18:43.000 --> 18:53.000]  I wanted to connect with Andrew Clark to see if there were any updates on the email tracking.\n",
            "[18:53.000 --> 18:54.000]  No?\n",
            "[18:54.000 --> 18:55.000]  Okay.\n",
            "[18:55.000 --> 19:03.000]  So I wanted to see if we could figure out what's causing the difference in the numbers and the registration.\n",
            "[19:03.000 --> 19:10.000]  I also have a couple of things to follow up with Madden about installing a pixel on the site\n",
            "[19:10.000 --> 19:22.000]  and a couple of other loose ends that I wanted to tie up.\n",
            "[19:22.000 --> 19:24.000]  You're still muted, Andrew.\n",
            "[19:24.000 --> 19:25.000]  Yeah.\n",
            "[19:25.000 --> 19:35.000]  I've got trips.\n",
            "Transcript saved to Whisper_transcript_en.txt\n",
            "Cleaned up audio file\n",
            "\n",
            "=== Processing Complete ===\n",
            "YouTube Transcript: Failed\n",
            "Whisper Transcript: Generated\n",
            "\n",
            "Check YTtranscript_en.txt and Whisper_transcript_en.txt for the results.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet youtube_transcript_api\n",
        "!pip install --quiet yt_dlp\n",
        "!pip install --quiet whisper-openai\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "# URL of the video to transcribe\n",
        "video_url = \"https://youtu.be/6qO1cHnJTz8\"\n",
        "\n",
        "import os\n",
        "import yt_dlp\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import whisper\n",
        "import re\n",
        "from datetime import timedelta\n",
        "import time\n",
        "import json\n",
        "\n",
        "def get_video_id(url):\n",
        "    \"\"\"Extract video ID from YouTube URL\"\"\"\n",
        "    print(\"Extracting video ID from URL...\")\n",
        "    patterns = [\n",
        "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
        "        r'(?:be\\/)([0-9A-Za-z_-]{11}).*'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, url)\n",
        "        if match:\n",
        "            video_id = match.group(1)\n",
        "            print(f\"Video ID extracted: {video_id}\")\n",
        "            return video_id\n",
        "    print(\"Failed to extract video ID.\")\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url, output_path='audio.mp3'):\n",
        "    \"\"\"Download audio from YouTube video\"\"\"\n",
        "    print(\"Downloading audio...\")\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'outtmpl': output_path.replace('.mp3', ''),\n",
        "        # Add rate limiting\n",
        "        'socket_timeout': 30,\n",
        "        'retries': 10,\n",
        "        'fragment_retries': 10,\n",
        "        'retry_sleep_functions': {'fragment': lambda n: 5 * (n + 1)},\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([video_url])\n",
        "        print(\"Audio downloaded successfully.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading audio: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "    \"\"\"Convert seconds to timestamp format\"\"\"\n",
        "    td = timedelta(seconds=float(seconds))\n",
        "    hours = td.seconds // 3600\n",
        "    minutes = (td.seconds % 3600) // 60\n",
        "    seconds = td.seconds % 60\n",
        "    if hours > 0:\n",
        "        return f\"[{hours:02d}:{minutes:02d}:{seconds:02d}]\"\n",
        "    return f\"[{minutes:02d}:{seconds:02d}]\"\n",
        "\n",
        "def get_youtube_transcript(video_id):\n",
        "    \"\"\"Get transcript from YouTube\"\"\"\n",
        "    try:\n",
        "        print(\"Checking available transcript languages...\")\n",
        "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "        available_languages = [tr.language_code for tr in transcript_list._transcripts.values()]\n",
        "        print(\"Available languages:\", available_languages)\n",
        "\n",
        "        print(\"Attempting to retrieve English transcript...\")\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "        print(\"English transcript retrieved successfully from YouTube.\")\n",
        "\n",
        "        formatted_transcript = []\n",
        "        for entry in transcript:\n",
        "            timestamp = format_timestamp(entry['start'])\n",
        "            formatted_transcript.append(f\"{timestamp} {entry['text']}\")\n",
        "\n",
        "        return \"\\n\".join(formatted_transcript)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting YouTube transcript: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_whisper_transcript(audio_path):\n",
        "    \"\"\"Generate transcript using Whisper\"\"\"\n",
        "    try:\n",
        "        print(\"Loading Whisper large model...\")\n",
        "        model = whisper.load_model(\"large\")\n",
        "\n",
        "        print(\"Transcribing audio with Whisper...\")\n",
        "        # Use chunks to handle long audio\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            task=\"transcribe\",\n",
        "            language=\"en\",\n",
        "            initial_prompt=\"This is an English transcript.\",  # Helps with English recognition\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        formatted_transcript = []\n",
        "        for segment in result[\"segments\"]:\n",
        "            timestamp = format_timestamp(segment[\"start\"])\n",
        "            formatted_transcript.append(f\"{timestamp} {segment['text']}\")\n",
        "\n",
        "        return \"\\n\".join(formatted_transcript)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Whisper transcription: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_transcript(transcript, filename):\n",
        "    \"\"\"Save transcript to file\"\"\"\n",
        "    if transcript:\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(transcript)\n",
        "            print(f\"Transcript saved to {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving transcript: {str(e)}\")\n",
        "    return False\n",
        "\n",
        "def process_video(video_url):\n",
        "    \"\"\"Process video and generate both types of transcripts\"\"\"\n",
        "    print(\"Starting video processing...\")\n",
        "\n",
        "    # Get video ID\n",
        "    video_id = get_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL\"\n",
        "\n",
        "    results = {\n",
        "        \"youtube_transcript\": None,\n",
        "        \"whisper_transcript\": None\n",
        "    }\n",
        "\n",
        "    # Get YouTube transcript\n",
        "    print(\"\\n=== Getting YouTube Transcript ===\")\n",
        "    yt_transcript = get_youtube_transcript(video_id)\n",
        "    if yt_transcript:\n",
        "        save_transcript(yt_transcript, 'YTtranscript_en.txt')\n",
        "        results[\"youtube_transcript\"] = yt_transcript\n",
        "\n",
        "    # Generate Whisper transcript\n",
        "    print(\"\\n=== Generating Whisper Transcript ===\")\n",
        "    if download_audio(video_url):\n",
        "        whisper_transcript = generate_whisper_transcript('audio.mp3')\n",
        "        if whisper_transcript:\n",
        "            save_transcript(whisper_transcript, 'Whisper_transcript_en.txt')\n",
        "            results[\"whisper_transcript\"] = whisper_transcript\n",
        "\n",
        "        # Clean up audio file\n",
        "        if os.path.exists(\"audio.mp3\"):\n",
        "            os.remove(\"audio.mp3\")\n",
        "            print(\"Cleaned up audio file\")\n",
        "\n",
        "    # Save combined results to JSON for comparison\n",
        "    with open('transcripts_info.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump({\n",
        "            \"video_id\": video_id,\n",
        "            \"youtube_transcript_available\": bool(results[\"youtube_transcript\"]),\n",
        "            \"whisper_transcript_available\": bool(results[\"whisper_transcript\"]),\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }, f, indent=4)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Process video and generate both transcripts\n",
        "print(\"Starting script...\")\n",
        "results = process_video(video_url)\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n=== Processing Complete ===\")\n",
        "print(\"YouTube Transcript:\", \"Generated\" if results[\"youtube_transcript\"] else \"Failed\")\n",
        "print(\"Whisper Transcript:\", \"Generated\" if results[\"whisper_transcript\"] else \"Failed\")\n",
        "print(\"\\nCheck YTtranscript_en.txt and Whisper_transcript_en.txt for the results.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Meeting Note Generator\n"
      ],
      "metadata": {
        "id": "AWX7hV58zqy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -q openai tiktoken pandas requests python-docx\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# Define folders\n",
        "transcripts_folder = '/content/'  # Update this path if your transcripts are in a different location\n",
        "meeting_notes_folder = '/content/drive/MyDrive/DMO/'  # Update this path as needed\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import logging\n",
        "from typing import List, Dict\n",
        "import tiktoken\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "from datetime import timedelta, datetime  # Added datetime for dynamic date\n",
        "import time\n",
        "from docx import Document  # For creating Word documents\n",
        "import getpass  # For secure API key input\n",
        "\n",
        "# Set up logger for debugging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = 'gpt-4o-mini'  # Ensure this is the correct model name\n",
        "MAX_MODEL_TOKENS = 128000\n",
        "MAX_RESPONSE_TOKENS = 10000\n",
        "\n",
        "# Retrieve API key\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('api_key')\n",
        "if not api_key:\n",
        "    api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Client Wrapper Class\n",
        "class ClientWrapper:\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "    def create_chat_completion(self, model: str, messages: List[Dict], temperature: float, max_tokens: int):\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "        response = requests.post(self.endpoint, headers=headers, json=payload)\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(\n",
        "                f\"OpenAI API request failed: {response.status_code} {response.text}\"\n",
        "            )\n",
        "        return response.json()\n",
        "\n",
        "client = ClientWrapper(api_key=api_key)\n",
        "\n",
        "# Token Counting Function\n",
        "def num_tokens_from_messages(messages: List[Dict], model: str) -> int:\n",
        "    \"\"\"Count the tokens of given messages using tiktoken.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        # Fallback encoding if the model is not recognized\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    if model.startswith(\"gpt-4\"):\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model.startswith(\"gpt-3.5-turbo\"):\n",
        "        tokens_per_message = 4\n",
        "        tokens_per_name = -1\n",
        "    else:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "\n",
        "    total_tokens = 0\n",
        "    for message in messages:\n",
        "        total_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            total_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                total_tokens += tokens_per_name\n",
        "    total_tokens += 2\n",
        "    return total_tokens\n",
        "\n",
        "# Content Chunking Function\n",
        "def chunk_text_content(text_content: str, max_chunk_tokens: int, model: str) -> List[str]:\n",
        "    \"\"\"Chunk text content into pieces that fit within the token limit.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    tokens = encoding.encode(text_content)\n",
        "    total_tokens = len(tokens)\n",
        "    logger.debug(f\"Total tokens in text content: {total_tokens}\")\n",
        "    chunks = []\n",
        "    start_idx = 0\n",
        "    while start_idx < total_tokens:\n",
        "        end_idx = min(start_idx + max_chunk_tokens, total_tokens)\n",
        "        chunk_tokens = tokens[start_idx:end_idx]\n",
        "        chunk = encoding.decode(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "        start_idx = end_idx\n",
        "    return chunks\n",
        "\n",
        "# Mappings Combination Function (Unused in this context)\n",
        "def combine_mappings(mappings_list: List[Dict]) -> Dict:\n",
        "    \"\"\"Combine multiple mappings into one.\"\"\"\n",
        "    combined_mapping = {}\n",
        "    for mapping in mappings_list:\n",
        "        for key, value in mapping.items():\n",
        "            if key == 'content_type':\n",
        "                combined_mapping[key] = value\n",
        "            else:\n",
        "                if isinstance(value, dict) and isinstance(combined_mapping.get(key), dict):\n",
        "                    # Merge nested dicts\n",
        "                    for sub_key, sub_val in value.items():\n",
        "                        if (sub_key not in combined_mapping[key] or\n",
        "                            not combined_mapping[key][sub_key] or\n",
        "                            combined_mapping[key][sub_key] == \"not found\"):\n",
        "                            combined_mapping[key][sub_key] = sub_val\n",
        "                else:\n",
        "                    if (key not in combined_mapping or\n",
        "                        not combined_mapping[key] or\n",
        "                        combined_mapping[key] == \"not found\"):\n",
        "                        combined_mapping[key] = value\n",
        "    return combined_mapping\n",
        "\n",
        "# System Prompt for Meeting Notes Generation\n",
        "system_prompt = \"\"\"\n",
        "You are an advanced AI assistant specialized in processing meeting transcripts to generate comprehensive and structured meeting notes. Your primary task is to convert raw transcript text from weekly marketing meetings of a destination marketing organization into organized meeting notes. These meetings involve team members discussing their current projects, schedules, agency collaborations, and other relevant topics.\n",
        "\n",
        "**Requirements for the Meeting Notes:**\n",
        "\n",
        "1. **Project Updates**\n",
        "   - **Project Name:** Clearly identify and state the name of each project being discussed to ensure all team members are aligned on which initiatives are being referenced.\n",
        "   - **Updates:** Provide a concise summary of the latest developments, progress, or changes related to each project. Include any new strategies implemented, milestones achieved, or adjustments made to project plans.\n",
        "   - **Milestones Tracking:** Highlight if any significant milestones that have been reached since the last meeting. This could include completed phases, successful launches, or noteworthy accomplishments that indicate project advancement. Outline the next set of milestones or actions required to move the project forward. Specify expected dates, responsible team members, and any preparatory steps needed to achieve these upcoming goals. If none were discussed simply enter 'Not Discussed' in this section.\n",
        "\n",
        "2. **To-Dos**\n",
        "   - **Action Items:** List specific tasks or actions that need to be completed. Clearly assign each task to the responsible team member or department to ensure accountability and ownership.\n",
        "   - **Teamwork Dependencies:** Identify any dependencies between tasks, team members, or departments that are necessary to accomplish the action items effectively. Highlight how different parts of the team need to coordinate to achieve these tasks. If none were mentioned simply enter 'None Discussed'\n",
        "   - **Deadlines and Timeframes:** Specify deadlines or target dates for each action item to ensure timely completion. Include any interim deadlines for sub-tasks if applicable to maintain progress tracking. If none were mentioned simply enter 'Not Discussed'.\n",
        "\n",
        "**Formatting Guidelines:**\n",
        "\n",
        "- **Title:** Begin with a clear title that includes the meeting name and date. For example: \"Marketing Standup\"\n",
        "\n",
        "- **Sections:** Organize the notes into the relevant main sections: Project Updates and To-Dos using bold headings for each section.\n",
        "\n",
        "- **Project Details:** Under Project Updates, list each project as a subheading followed by bullet points detailing the updates.\n",
        "\n",
        "- **Action Items:** Under To-Dos, list each action item with clear attribution to the responsible team member. Indicate any dependencies, collaborations, and deadlines mentioned.\n",
        "\n",
        "**Processing Instructions:**\n",
        "\n",
        "1. **Input:** You will receive transcript text data with timestamps from a weekly marketing meeting.\n",
        "\n",
        "2. **Extraction:**\n",
        "   - Identify and extract discussions related to project updates, ensuring each project is clearly named and its updates are succinctly summarized.\n",
        "   - Determine action items and assign them to the appropriate team members, noting any dependencies or required collaborations.\n",
        "   - Summarize the overall meeting and list any meetings scheduled for the upcoming week with their respective times.\n",
        "\n",
        "3. **Output:** Generate the meeting notes following the structure and formatting guidelines provided above. Ensure clarity, conciseness, and organization to facilitate easy reference and action by team members.\n",
        "\n",
        "**Additional Notes:**\n",
        "\n",
        "- Maintain consistency in formatting to ensure readability.\n",
        "- Use bullet points for clarity and brevity.\n",
        "- Ensure all relevant information from the transcript is captured accurately in the meeting notes.\n",
        "- Omit any irrelevant or redundant information that does not pertain to the three main sections.\n",
        "\n",
        "By adhering to these guidelines, you will produce effective and actionable meeting notes that enhance team collaboration and project management.\n",
        "\"\"\"\n",
        "\n",
        "# User Prompt Template\n",
        "prompt_user_template = \"\"\"\n",
        "Analyze this transcript and generate the meeting notes as specified:\n",
        "\n",
        "<Transcript Here>\n",
        "\"\"\"\n",
        "\n",
        "# Function to Generate Meeting Notes from Transcript\n",
        "def generate_meeting_notes(transcript: str, meeting_date: str, meeting_name: str = \"Marketing Standup\") -> str:\n",
        "    \"\"\"Generate structured meeting notes from a transcript.\"\"\"\n",
        "    system_msg = system_prompt\n",
        "    user_msg = prompt_user_template.replace(\"<Transcript Here>\", transcript)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_msg},\n",
        "        {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        "\n",
        "    total_tokens = num_tokens_from_messages(messages, MODEL_NAME)\n",
        "    max_prompt_tokens = MAX_MODEL_TOKENS - MAX_RESPONSE_TOKENS\n",
        "\n",
        "    if total_tokens > max_prompt_tokens:\n",
        "        logger.debug(f\"Transcript exceeds token limit. Chunking necessary.\")\n",
        "        max_chunk_tokens = max_prompt_tokens // 2  # Adjust as needed\n",
        "        chunks = chunk_text_content(transcript, max_chunk_tokens, MODEL_NAME)\n",
        "    else:\n",
        "        chunks = [transcript]\n",
        "\n",
        "    meeting_notes_sections = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        logger.debug(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
        "        current_user_msg = prompt_user_template.replace(\"<Transcript Here>\", chunk)\n",
        "        current_messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": current_user_msg}\n",
        "        ]\n",
        "        try:\n",
        "            response = client.create_chat_completion(\n",
        "                model=MODEL_NAME,\n",
        "                messages=current_messages,\n",
        "                temperature=0.3,\n",
        "                max_tokens=MAX_RESPONSE_TOKENS\n",
        "            )\n",
        "            content = response['choices'][0]['message']['content'].strip()\n",
        "            meeting_notes_sections.append(content)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating meeting notes for chunk {i+1}: {e}\")\n",
        "            meeting_notes_sections.append(f\"Error processing chunk {i+1}\")\n",
        "\n",
        "    # Combine all sections\n",
        "    final_meeting_notes = \"\\n\\n\".join(meeting_notes_sections)\n",
        "\n",
        "    # Add Title\n",
        "    final_meeting_notes = f\"{meeting_name} - {meeting_date}\\n\\n\" + final_meeting_notes\n",
        "\n",
        "    return final_meeting_notes\n",
        "\n",
        "# Function to Load Existing Transcript\n",
        "def load_existing_transcript(transcripts_folder: str) -> str:\n",
        "    \"\"\"Load the existing transcript from the transcripts folder.\"\"\"\n",
        "    yt_transcript_path = os.path.join(transcripts_folder, 'YTtranscript_en.txt')\n",
        "    whisper_transcript_path = os.path.join(transcripts_folder, 'Whisper_transcript_en.txt')\n",
        "\n",
        "    if os.path.exists(yt_transcript_path):\n",
        "        print(\"Loading YouTube transcript...\")\n",
        "        with open(yt_transcript_path, 'r', encoding='utf-8') as f:\n",
        "            transcript = f.read()\n",
        "        return transcript\n",
        "    elif os.path.exists(whisper_transcript_path):\n",
        "        print(\"Loading Whisper transcript...\")\n",
        "        with open(whisper_transcript_path, 'r', encoding='utf-8') as f:\n",
        "            transcript = f.read()\n",
        "        return transcript\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No transcript file found in the transcripts folder.\")\n",
        "\n",
        "# Function to Save Meeting Notes as a Word Document\n",
        "def save_meeting_notes_as_word(meeting_notes: str, meeting_notes_path: str):\n",
        "    \"\"\"Save the meeting notes as a Word document.\"\"\"\n",
        "    try:\n",
        "        document = Document()\n",
        "        lines = meeting_notes.split('\\n')\n",
        "        for line in lines:\n",
        "            stripped_line = line.strip()\n",
        "            if stripped_line.startswith(\"**\") and stripped_line.endswith(\"**\"):\n",
        "                # Add as a heading\n",
        "                heading_text = stripped_line.strip(\"*\")\n",
        "                document.add_heading(heading_text, level=1)\n",
        "            elif stripped_line.startswith(\"*\") and stripped_line.endswith(\"*\"):\n",
        "                # Add as a subheading\n",
        "                subheading_text = stripped_line.strip(\"*\")\n",
        "                document.add_heading(subheading_text, level=2)\n",
        "            else:\n",
        "                # Add as a paragraph\n",
        "                document.add_paragraph(line)\n",
        "        document.save(meeting_notes_path)\n",
        "        print(f\"Meeting notes saved to {meeting_notes_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving meeting notes as Word document: {str(e)}\")\n",
        "\n",
        "# Main Processing Function\n",
        "def main():\n",
        "    # Dynamically assign today's date in MM/DD/YYYY format\n",
        "    meeting_date = datetime.now().strftime(\"%m/%d/%Y\")\n",
        "\n",
        "    try:\n",
        "        # Load existing transcript\n",
        "        print(\"Loading existing transcript...\")\n",
        "        transcript = load_existing_transcript(transcripts_folder)\n",
        "    except FileNotFoundError as e:\n",
        "        print(str(e))\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the transcript: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Generate meeting notes\n",
        "    print(\"\\n=== Generating Meeting Notes ===\")\n",
        "    meeting_notes = generate_meeting_notes(transcript, meeting_date)\n",
        "\n",
        "    # Define meeting notes filename\n",
        "    meeting_notes_filename = f\"Marketing_Standup_{meeting_date.replace('/', '-')}.docx\"\n",
        "    meeting_notes_path = os.path.join(meeting_notes_folder, meeting_notes_filename)\n",
        "\n",
        "    # Save meeting notes as a Word document\n",
        "    save_meeting_notes_as_word(meeting_notes, meeting_notes_path)\n",
        "\n",
        "    # Optionally, display the meeting notes\n",
        "    print(\"\\n=== Generated Meeting Notes ===\")\n",
        "    print(meeting_notes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwCPCthn1Dql",
        "outputId": "1708dcde-4d2c-44c4-9363-cd286f89b3e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "Loading existing transcript...\n",
            "Loading Whisper transcript...\n",
            "\n",
            "=== Generating Meeting Notes ===\n",
            "Meeting notes saved to /content/drive/MyDrive/DMO/Marketing_Standup_12-16-2024.docx\n",
            "\n",
            "=== Generated Meeting Notes ===\n",
            "Marketing Standup - 12/16/2024\n",
            "\n",
            "# Marketing Standup - [Date Not Specified]\n",
            "\n",
            "## Project Updates\n",
            "\n",
            "### Google Ads\n",
            "- **Updates:** Working on the statement of work for Google Ads.\n",
            "- **Milestones Tracking:** Follow-up needed with Joanne regarding the Explore Georgia ad. New ads are available in the correct dimensions.\n",
            "\n",
            "### Clip Trips\n",
            "- **Updates:** Meeting held with Clip Trips representatives. Awaiting further communication from Travis to discuss next steps.\n",
            "\n",
            "### Max Award Submission\n",
            "- **Updates:** Currently gathering numbers from Telemundo and the website. A video script is being developed to present the data.\n",
            "\n",
            "### Lovefest\n",
            "- **Updates:** Nobu has accepted the bid to host Lovefest, and a favorable rate for accommodations has been secured. The contract will be signed today.\n",
            "- **Milestones Tracking:** Next meeting scheduled for Thursday.\n",
            "\n",
            "### ATL Unguided\n",
            "- **Updates:** Interest from potential partners regarding brochures has been noted. Further updates will be provided to Joanne.\n",
            "\n",
            "### Cookbook Sales\n",
            "- **Updates:** Ongoing discussions regarding retail sales possibilities and commissions for local bookshops. Need to confirm details with Andrew and Jalen.\n",
            "\n",
            "### CFP Event\n",
            "- **Updates:** Coordination ongoing with CFP for marketing efforts and asset provision for participating chefs.\n",
            "\n",
            "### Promo Items\n",
            "- **Updates:** Higher-end branded lanyards are being ordered for staff use at industry events. A virtual mockup will be shared once available.\n",
            "\n",
            "## To-Dos\n",
            "\n",
            "- **Action Item:** Complete the statement of work for Google Ads.  \n",
            "  **Responsible:** Andrew  \n",
            "  **Deadline:** This week\n",
            "\n",
            "- **Action Item:** Follow up with Joanne regarding the Explore Georgia ad.  \n",
            "  **Responsible:** Andrew  \n",
            "  **Deadline:** Not Discussed\n",
            "\n",
            "- **Action Item:** Finalize the video script for the Max Award submission.  \n",
            "  **Responsible:** Team Member (Name Not Specified)  \n",
            "  **Deadline:** This week\n",
            "\n",
            "- **Action Item:** Sign the contract with Nobu for Lovefest.  \n",
            "  **Responsible:** Team Member (Name Not Specified)  \n",
            "  **Deadline:** Today\n",
            "\n",
            "- **Action Item:** Provide updates on brochure interest for ATL Unguided to Joanne.  \n",
            "  **Responsible:** Team Member (Name Not Specified)  \n",
            "  **Deadline:** Not Discussed\n",
            "\n",
            "- **Action Item:** Confirm retail sales possibilities and commissions for the Cookbook project.  \n",
            "  **Responsible:** Andrew, Jalen  \n",
            "  **Deadline:** This week\n",
            "\n",
            "- **Action Item:** Gather headshots for CFP event marketing.  \n",
            "  **Responsible:** Lauren  \n",
            "  **Deadline:** Before the next call with Mary Beth\n",
            "\n",
            "- **Action Item:** Review and align feedback on the meetings proposal from SparkLoft.  \n",
            "  **Responsible:** Andrew, Sharitha  \n",
            "  **Deadline:** Today at 1 PM\n",
            "\n",
            "- **Action Item:** Order higher-end branded lanyards for staff use.  \n",
            "  **Responsible:** Team Member (Name Not Specified)  \n",
            "  **Deadline:** Before Lovefest\n",
            "\n",
            "- **Action Item:** Investigate email tracking discrepancies with Andrew Clark.  \n",
            "  **Responsible:** Alex  \n",
            "  **Deadline:** Not Discussed\n",
            "\n",
            "### Teamwork Dependencies\n",
            "- Follow-up on the Explore Georgia ad is dependent on Andrew's communication with Joanne.\n",
            "- The Max Award submission is contingent on receiving numbers from Telemundo and the website.\n",
            "- Coordination for the CFP event requires collaboration with multiple team members for asset provision.\n",
            "\n",
            "### Deadlines and Timeframes\n",
            "- Most action items have immediate deadlines within the week or specific dates mentioned (e.g., today for the Nobu contract). Further deadlines are not discussed.\n",
            "\n",
            "---\n",
            "\n",
            "**Note:** The date of the meeting was not specified in the transcript. Please update accordingly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yULupTsJ2nX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}